{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295badff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Base fictícia -> dados não existem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88319a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Using cached pyspark-3.1.2.tar.gz (212.4 MB)\n",
      "Collecting py4j==0.10.9\n",
      "  Using cached py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py): started\n",
      "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=362309db358c55d71d82ce8ccb0db6b40f922b0adbf6a0c1706cde378c4e1e50\n",
      "  Stored in directory: c:\\users\\rosan\\appdata\\local\\pip\\cache\\wheels\\df\\88\\9e\\58ef1f74892fef590330ca0830b5b6d995ba29b44f977b3926\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9 pyspark-3.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ccce913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession as spark, SQLContext\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import types as t\n",
    "from pyspark.sql import Row\n",
    "\n",
    "from datetime import datetime,date\n",
    "#aí configura conexão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58a43f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para ler uma base:\n",
    "base = spark.read.table('database.base')\n",
    "\n",
    "#Para filtrar use Filter\n",
    "#No caso a seguir, filtra-se registros com a data de hoje\n",
    "#O f.col referencia a coluna\n",
    "hoje = datetime.now().date()\n",
    "base = spark.read.table('database.base').filter(f.col('data') == hoje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9d10d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para criar uma nova coluna use WithColumn(nome da coluna, condição)\n",
    "#Caso tenha que usar if, use f.when, o else é otherwise\n",
    "\n",
    "base_feminina = base.withColumn('flag_genero',f.when(f.col('genero') == \"M\",'sim').otherwise('não'))\\\n",
    "                    .filter(f.col('flag_genero') == 'sim')\n",
    "\n",
    "#Para selecionar colunas use Select\n",
    "base_feminina.select('flag_genero','genero').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3de091",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cruzamento entre tabelas\n",
    "#duas tabelas: vendas e produtos\n",
    "\n",
    "#Schema tb_vendas:\n",
    "#        | id_venda | id_produto | quantidade | valor |\n",
    "#          ----------------------------------------\n",
    "#            int    |    int     |     int    | double\n",
    "#          ----------------------------------------\n",
    "\n",
    "#Schema tb_produtos:\n",
    "#        | id_produto | valor  | nome_produto |\n",
    "#          ----------------------------------\n",
    "#             int     | double |    string    |\n",
    "#          ----------------------------------\n",
    "\n",
    "#o cruzamento será da tb_vendas com a tb_produtos para pegar nome do produto para cada venda, então será um left join\n",
    "tb_vendas.join(tb_produtos, tb_vendas['id_produto'] == tb_produtos['id_produto'], 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c275eaeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32aa9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#criando base\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'a':[1,2,3,4],\n",
    "    'b':[10,15,20,13],\n",
    "    'c':['Marilu','Lucas','Luna','Bete'],\n",
    "    'd':[date(2021,9,10),date(2021,9,8),date(2021,9,12),date(2021,9,25)]\n",
    "})\n",
    "\n",
    "base = spark.createDataFrame(df)\n",
    "base"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
